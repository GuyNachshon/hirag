# DotsOCR Vision Server Dockerfile - Based on official repo
FROM vllm/vllm-openai:v0.9.1

# Install required dependencies as per their Dockerfile
RUN pip3 install flash_attn==2.8.0.post2
RUN pip3 install transformers==4.51.3

# Set working directory to match their setup
WORKDIR /workspace

# Download model weights using their exact approach
RUN python3 -c "\
from huggingface_hub import snapshot_download; \
import os; \
model_dir = './weights/DotsOCR'; \
os.makedirs(model_dir, exist_ok=True); \
snapshot_download(\
    repo_id='rednote-hilab/dots.ocr', \
    local_dir=model_dir, \
    local_dir_use_symlinks=False, \
    resume_download=True\
); \
print(f'Model downloaded to {model_dir}')\
"

# Set environment variables matching their setup
ENV PYTHONPATH="/workspace/weights:$PYTHONPATH"
ENV CUDA_VISIBLE_DEVICES=0

# Expose port
EXPOSE 8000

# Create startup script with proper newlines using printf
RUN printf '#!/bin/bash\nset -ex\necho "--- Starting DotsOCR server ---"\necho "Modifying vllm entrypoint..."\nsed -i "/^from vllm\\.entrypoints\\.cli\\.main import main/a from DotsOCR import modeling_dots_ocr_vllm" $(which vllm)\necho "vllm script patched successfully"\ngrep -A 1 "from vllm.entrypoints.cli.main import main" $(which vllm)\necho "Starting server..."\nexec vllm serve /workspace/weights/DotsOCR \\\\\n    --tensor-parallel-size 1 \\\\\n    --gpu-memory-utilization 0.95 \\\\\n    --chat-template-content-format string \\\\\n    --served-model-name model \\\\\n    --trust-remote-code \\\\\n    --host 0.0.0.0 \\\\\n    --port 8000\n' > /workspace/start_dots_ocr.sh

RUN chmod +x /workspace/start_dots_ocr.sh

# Override the vLLM entrypoint
ENTRYPOINT []
CMD ["/workspace/start_dots_ocr.sh"]