# DotsOCR Vision Server Dockerfile - Based on official repo
FROM vllm/vllm-openai:v0.9.1

# Install required dependencies as per their Dockerfile
RUN pip3 install flash_attn==2.8.0.post2
RUN pip3 install transformers==4.51.3

# Set working directory to match their setup
WORKDIR /workspace

# Download model weights using their exact approach
RUN python3 -c "\
from huggingface_hub import snapshot_download; \
import os; \
model_dir = './weights/DotsOCR'; \
os.makedirs(model_dir, exist_ok=True); \
snapshot_download(\
    repo_id='rednote-hilab/dots.ocr', \
    local_dir=model_dir, \
    local_dir_use_symlinks=False, \
    resume_download=True\
); \
print(f'Model downloaded to {model_dir}')\
"

# Set environment variables matching their setup
ENV PYTHONPATH="/workspace/weights:$PYTHONPATH"
ENV CUDA_VISIBLE_DEVICES=0

# Expose port
EXPOSE 8000

# Create startup script properly
RUN cat > /workspace/start_dots_ocr.sh << 'EOF'
#!/bin/bash
set -ex
echo "--- Starting DotsOCR server ---"
echo "Modifying vllm entrypoint..."
sed -i "/^from vllm\.entrypoints\.cli\.main import main/a from DotsOCR import modeling_dots_ocr_vllm" $(which vllm)
echo "vllm script patched successfully"
grep -A 1 "from vllm.entrypoints.cli.main import main" $(which vllm)
echo "Starting server..."
exec vllm serve /workspace/weights/DotsOCR \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.95 \
    --chat-template-content-format string \
    --served-model-name model \
    --trust-remote-code \
    --host 0.0.0.0 \
    --port 8000
EOF

RUN chmod +x /workspace/start_dots_ocr.sh

# Override the vLLM entrypoint
ENTRYPOINT []
CMD ["/workspace/start_dots_ocr.sh"]