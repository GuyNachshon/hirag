# DotsOCR Vision Server Dockerfile - Based on official repo
FROM vllm/vllm-openai:v0.9.1

# Install required dependencies as per their Dockerfile
RUN pip3 install flash_attn==2.8.0.post2
RUN pip3 install transformers==4.51.3

# Set working directory to match their setup
WORKDIR /workspace

# Download model weights using their exact approach
RUN python3 -c "\
from huggingface_hub import snapshot_download; \
import os; \
model_dir = './weights/DotsOCR'; \
os.makedirs(model_dir, exist_ok=True); \
snapshot_download(\
    repo_id='rednote-hilab/dots.ocr', \
    local_dir=model_dir, \
    local_dir_use_symlinks=False, \
    resume_download=True\
); \
print(f'Model downloaded to {model_dir}')\
"

# Set environment variables matching their setup
ENV PYTHONPATH="/workspace/weights:$PYTHONPATH"
ENV CUDA_VISIBLE_DEVICES=0

# Expose port
EXPOSE 8000

# Create and use startup script
RUN echo '#!/bin/bash\n\
set -ex\n\
echo "--- Starting DotsOCR server ---"\n\
echo "Modifying vllm entrypoint..."\n\
sed -i "/^from vllm\\.entrypoints\\.cli\\.main import main/a from DotsOCR import modeling_dots_ocr_vllm" $(which vllm)\n\
echo "vllm script patched successfully"\n\
grep -A 1 "from vllm.entrypoints.cli.main import main" $(which vllm)\n\
echo "Starting server..."\n\
exec vllm serve /workspace/weights/DotsOCR \\\n\
    --tensor-parallel-size 1 \\\n\
    --gpu-memory-utilization 0.95 \\\n\
    --chat-template-content-format string \\\n\
    --served-model-name model \\\n\
    --trust-remote-code \\\n\
    --host 0.0.0.0 \\\n\
    --port 8000' > /workspace/start_dots_ocr.sh

RUN chmod +x /workspace/start_dots_ocr.sh

# Override the vLLM entrypoint
ENTRYPOINT []
CMD ["/workspace/start_dots_ocr.sh"]