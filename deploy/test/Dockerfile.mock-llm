# Mock LLM Service for Testing (No GPU Required)
FROM python:3.9-slim

WORKDIR /app

# Install FastAPI and dependencies
RUN pip install fastapi uvicorn

# Create mock LLM service
RUN cat > mock_llm.py << 'EOF'
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import random
import time
import json
import asyncio

app = FastAPI()

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 100
    stream: Optional[bool] = False

class EmbeddingRequest(BaseModel):
    model: str
    input: List[str]

# Mock responses
MOCK_RESPONSES = [
    "This is a mock response for testing purposes.",
    "The system is working correctly in test mode.",
    "I'm a mock LLM service running without GPU.",
    "Testing the RAG system integration.",
    "Mock response: Everything appears to be functioning properly."
]

@app.get("/health")
async def health():
    return {"status": "healthy", "service": "mock-llm", "gpu": False}

@app.get("/v1/models")
async def list_models():
    return {
        "data": [
            {"id": "mock-model", "object": "model", "owned_by": "mock"},
            {"id": "test-model", "object": "model", "owned_by": "test"}
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    # Simulate some processing time
    await asyncio.sleep(0.5)
    
    # Get a random mock response
    response_text = random.choice(MOCK_RESPONSES)
    
    if request.messages:
        last_message = request.messages[-1].content
        response_text = f"Mock response to: '{last_message[:50]}...'. {response_text}"
    
    if request.stream:
        async def generate():
            words = response_text.split()
            for word in words:
                chunk = {
                    "choices": [{
                        "delta": {"content": word + " "},
                        "index": 0
                    }]
                }
                yield f"data: {json.dumps(chunk)}\n\n"
                await asyncio.sleep(0.05)
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(generate(), media_type="text/event-stream")
    
    return {
        "id": "mock-" + str(int(time.time())),
        "object": "chat.completion",
        "created": int(time.time()),
        "model": request.model,
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": response_text
            },
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": 10,
            "completion_tokens": 20,
            "total_tokens": 30
        }
    }

@app.post("/v1/embeddings")
async def create_embeddings(request: EmbeddingRequest):
    # Generate mock embeddings
    embeddings = []
    for text in request.input:
        # Create a simple mock embedding (768 dimensions)
        embedding = [random.random() for _ in range(768)]
        embeddings.append(embedding)
    
    return {
        "object": "list",
        "data": [
            {
                "object": "embedding",
                "embedding": emb,
                "index": i
            } for i, emb in enumerate(embeddings)
        ],
        "model": request.model,
        "usage": {
            "prompt_tokens": len(request.input) * 10,
            "total_tokens": len(request.input) * 10
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
EOF

EXPOSE 8000

CMD ["python", "mock_llm.py"]