# Optimized Whisper for Offline Deployment
# Combines Ivrit-AI optimizations with offline support

FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsndfile1 \
    curl \
    build-essential \
    git \
    && rm -rf /var/lib/apt/lists/*

# Environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    HF_HOME=/root/.cache/huggingface \
    TRANSFORMERS_CACHE=/root/.cache/huggingface \
    HF_HUB_OFFLINE=1 \
    TRANSFORMERS_OFFLINE=1 \
    DEVICE=cuda \
    TORCH_DEVICE=cuda:0

# Install PyTorch with CUDA support
RUN pip install --no-cache-dir \
    torch==2.3.0 \
    torchaudio==2.3.0 \
    --index-url https://download.pytorch.org/whl/cu121

# Install dependencies
RUN pip install --no-cache-dir \
    fastapi==0.104.1 \
    uvicorn==0.24.0 \
    transformers==4.41.2 \
    accelerate==0.25.0 \
    safetensors>=0.4.1 \
    librosa==0.10.2 \
    python-multipart==0.0.6 \
    numpy==1.24.3 \
    optimum \
    ctranslate2 \
    faster-whisper

# Pre-download models for offline use
ARG WHISPER_MODEL="ivrit-ai/whisper-large-v3"
ENV MODEL_NAME=${WHISPER_MODEL}

# Download both standard and CT2 optimized versions
RUN python3 -c "\
import os; \
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq; \
import torch; \
model_name = os.environ.get('MODEL_NAME', 'ivrit-ai/whisper-large-v3'); \
print(f'Pre-downloading Whisper model: {model_name}'); \
processor = AutoProcessor.from_pretrained(model_name); \
model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, torch_dtype=torch.float16); \
print(f'Model {model_name} downloaded successfully'); \
"

# Create optimized service
RUN cat > whisper_service_optimized.py << 'EOF'
import os
import torch
import logging
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, pipeline
import numpy as np
import librosa
from typing import Optional
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Optimized Whisper Service")

# Global variables
model = None
processor = None
pipe = None

def load_model():
    global model, processor, pipe

    model_name = os.environ.get('MODEL_NAME', 'ivrit-ai/whisper-large-v3')
    device = os.environ.get('DEVICE', 'cuda' if torch.cuda.is_available() else 'cpu')

    logger.info(f"Loading model: {model_name}")
    logger.info(f"Device: {device}")
    logger.info(f"CUDA available: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

    try:
        # Load with optimizations
        processor = AutoProcessor.from_pretrained(
            model_name,
            local_files_only=True  # Force offline mode
        )

        model = AutoModelForSpeechSeq2Seq.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
            low_cpu_mem_usage=True,
            use_safetensors=True,
            local_files_only=True  # Force offline mode
        ).to(device)

        # Create pipeline with optimizations
        pipe = pipeline(
            "automatic-speech-recognition",
            model=model,
            tokenizer=processor.tokenizer,
            feature_extractor=processor.feature_extractor,
            max_new_tokens=128,
            chunk_length_s=30,
            batch_size=16,
            return_timestamps=True,
            torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
            device=device,
        )

        logger.info("Model loaded successfully")

    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        raise

@app.on_event("startup")
async def startup_event():
    load_model()

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model": os.environ.get('MODEL_NAME', 'ivrit-ai/whisper-large-v3'),
        "device": os.environ.get('DEVICE', 'cuda' if torch.cuda.is_available() else 'cpu'),
        "cuda_available": torch.cuda.is_available(),
        "offline_mode": bool(os.environ.get('HF_HUB_OFFLINE'))
    }

@app.post("/transcribe")
async def transcribe(file: UploadFile = File(...)):
    if not file:
        raise HTTPException(status_code=400, detail="No file provided")

    try:
        # Read audio file
        audio_bytes = await file.read()

        # Load audio with librosa
        audio_array, sample_rate = librosa.load(
            io.BytesIO(audio_bytes),
            sr=16000  # Whisper expects 16kHz
        )

        # Transcribe with GPU acceleration
        start_time = time.time()

        with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():
            result = pipe(audio_array, generate_kwargs={"language": "he"})

        transcription_time = time.time() - start_time

        return JSONResponse(content={
            "text": result["text"],
            "language": "he",
            "processing_time": transcription_time,
            "device": str(pipe.device),
            "chunks": result.get("chunks", [])
        })

    except Exception as e:
        logger.error(f"Transcription error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/transcribe_url")
async def transcribe_url(url: str):
    """For compatibility with Ivrit-AI RunPod API"""
    raise HTTPException(
        status_code=501,
        detail="URL transcription not supported in offline mode"
    )

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8004)
EOF

# Create startup script
RUN cat > start_whisper.sh << 'EOF'
#!/bin/bash
echo "Starting Optimized Whisper Service"
echo "Model: $MODEL_NAME"
echo "Device: $DEVICE"
echo "Offline Mode: $HF_HUB_OFFLINE"

# Ensure GPU is accessible
if [ "$DEVICE" = "cuda" ]; then
    python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
    nvidia-smi 2>/dev/null || echo "nvidia-smi not available"
fi

# Start service
exec python whisper_service_optimized.py
EOF

RUN chmod +x start_whisper.sh

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8004/health || exit 1

EXPOSE 8004

CMD ["./start_whisper.sh"]