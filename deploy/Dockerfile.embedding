# Embedding Server Dockerfile  
FROM vllm/vllm-openai:latest

WORKDIR /app

# Set environment variables
ENV CUDA_VISIBLE_DEVICES=0
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn

# Expose port
EXPOSE 8000

# Pre-download embedding model during build (for offline deployment)
ARG EMBEDDING_MODEL="Qwen/Qwen2-0.5B-Instruct"
ENV MODEL_NAME=${EMBEDDING_MODEL}

# Pre-download model during build
RUN python -c "
import os
from huggingface_hub import snapshot_download
model_name = os.environ.get('MODEL_NAME', 'Qwen/Qwen2-0.5B-Instruct')
print(f'Pre-downloading embedding model: {model_name}')
try:
    snapshot_download(
        repo_id=model_name,
        cache_dir='/root/.cache/huggingface/hub',
        local_files_only=False,
        resume_download=True
    )
    print(f'Embedding model {model_name} downloaded successfully')
except Exception as e:
    print(f'Error downloading embedding model: {e}')
    exit(1)
"

# Create startup script
RUN echo '#!/bin/bash\n\
MODEL=${MODEL_NAME:-"Qwen/Qwen2-0.5B-Instruct"}\n\
\n\
echo "Starting Embedding server with model: $MODEL"\n\
echo "Model is pre-downloaded for offline deployment"\n\
\n\
vllm serve $MODEL \\\n\
  --host 0.0.0.0 \\\n\
  --port 8000 \\\n\
  --trust-remote-code' > /app/start_embedding.sh

RUN chmod +x /app/start_embedding.sh

CMD ["/app/start_embedding.sh"]