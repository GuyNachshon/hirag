# gpt-oss LLM Server Dockerfile
FROM vllm/vllm-openai:latest

WORKDIR /app

# Install special gpt-oss vLLM version
RUN pip install --pre vllm==0.10.1+gptoss \
    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \
    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \
    --index-strategy unsafe-best-match

# Set environment variables
ENV CUDA_VISIBLE_DEVICES=0

# Expose port
EXPOSE 8000

# Model will be downloaded automatically by vLLM on first run
ARG GPT_OSS_MODEL="openai/gpt-oss-20b"
ENV MODEL_NAME=${GPT_OSS_MODEL}

# Create startup script for gpt-oss
RUN echo '#!/bin/bash\n\
MODEL=${MODEL_NAME:-"openai/gpt-oss-20b"}\n\
TENSOR_PARALLEL=${TENSOR_PARALLEL:-1}\n\
GPU_MEMORY=${GPU_MEMORY:-0.8}\n\
\n\
echo "Starting gpt-oss server with model: $MODEL"\n\
echo "vLLM will automatically download the model on first run"\n\
\n\
vllm serve $MODEL \\\n\
  --host 0.0.0.0 \\\n\
  --port 8000 \\\n\
  --tensor-parallel-size $TENSOR_PARALLEL \\\n\
  --gpu-memory-utilization $GPU_MEMORY \\\n\
  --trust-remote-code' > /app/start_gpt_oss.sh

RUN chmod +x /app/start_gpt_oss.sh

CMD ["/app/start_gpt_oss.sh"]