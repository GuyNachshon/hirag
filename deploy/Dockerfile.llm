# gpt-oss LLM Server Dockerfile (Large Model) - Optimized
FROM vllm/vllm-openai:latest

WORKDIR /app

# Set environment variables
ENV CUDA_VISIBLE_DEVICES=0
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn

# Pre-download gpt-oss model during build (for offline deployment)
ARG GPT_OSS_MODEL="openai/gpt-oss-20b"
ENV MODEL_NAME=${GPT_OSS_MODEL}

# Combined optimization: Install gpt-oss vLLM, download model, create script, and cleanup in single layer
RUN set -e && \
    # Install gpt-oss vLLM version \
    pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir --pre vllm==0.10.1+gptoss \
    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \
    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 && \
    # Pre-download model \
    python3 -c "\
import os; \
from huggingface_hub import snapshot_download; \
model_name = os.environ.get('MODEL_NAME', 'openai/gpt-oss-20b'); \
print(f'Pre-downloading model: {model_name}'); \
snapshot_download(repo_id=model_name, cache_dir='/root/.cache/huggingface/hub', local_files_only=False, resume_download=True); \
print(f'Model {model_name} downloaded successfully')" && \
    # Create startup script \
    printf '#!/bin/bash\nMODEL=${MODEL_NAME:-"openai/gpt-oss-20b"}\nTENSOR_PARALLEL=${TENSOR_PARALLEL:-1}\nGPU_MEMORY=${GPU_MEMORY:-0.8}\n\necho "Starting gpt-oss server with model: $MODEL"\necho "Model is pre-downloaded for offline deployment"\n\nvllm serve $MODEL --host 0.0.0.0 --port 8000 --tensor-parallel-size $TENSOR_PARALLEL --gpu-memory-utilization $GPU_MEMORY --trust-remote-code\n' > /app/start_gpt_oss.sh && \
    chmod +x /app/start_gpt_oss.sh && \
    # Cleanup to reduce image size \
    pip cache purge && \
    rm -rf /tmp/* && \
    rm -rf /var/cache/* && \
    rm -rf /root/.cache/pip && \
    # Keep HuggingFace cache but remove unnecessary files \
    find /root/.cache/huggingface -name "*.tmp*" -delete 2>/dev/null || true && \
    # Remove build artifacts and unnecessary packages \
    apt-get autoremove -y && \
    apt-get autoclean && \
    rm -rf /var/lib/apt/lists/*

# Expose port
EXPOSE 8000

# Override the vLLM entrypoint
ENTRYPOINT []
CMD ["/app/start_gpt_oss.sh"]