# Whisper Transcription Server Dockerfile - Optimized
# Hebrew-optimized transcription using ivrit-ai/whisper-large-v3-turbo

FROM python:3.11-slim

WORKDIR /app

# Install system dependencies for audio processing
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsndfile1 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# Pre-download model during build (for offline deployment)
ARG WHISPER_MODEL="openai/whisper-large-v3"
ENV MODEL_NAME=${WHISPER_MODEL}

# Install Python dependencies and pre-download model in single layer
RUN set -e && \
    pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir \
        fastapi==0.104.1 \
        uvicorn==0.24.0 \
        faster-whisper==1.0.1 \
        python-multipart==0.0.6 \
        numpy==1.24.3 && \
    python3 -c "\
import os; \
from faster_whisper import WhisperModel; \
model_name = os.environ.get('MODEL_NAME', 'openai/whisper-large-v3'); \
print(f'Pre-downloading Whisper model: {model_name}'); \
model = WhisperModel(model_name, device='cpu', compute_type='int8'); \
print(f'Whisper model {model_name} downloaded successfully')" && \
    pip cache purge && \
    rm -rf /tmp/* && \
    rm -rf /var/cache/* && \
    rm -rf /root/.cache/pip && \
    apt-get autoremove -y && \
    apt-get autoclean

# Copy service files
COPY deploy/whisper_service.py /app/
COPY deploy/start_whisper.sh /app/
RUN chmod +x /app/start_whisper.sh

# Expose port
EXPOSE 8004

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:8004/health || exit 1

CMD ["/app/start_whisper.sh"]