# RunPod-optimized vLLM Server with embedded models
FROM runpod/base:0.4.0-cuda12.1.0

# Install Python and system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-pip \
    python3.11-dev \
    curl \
    git \
    ffmpeg \
    && ln -s /usr/bin/python3.11 /usr/bin/python \
    && ln -s /usr/bin/pip3 /usr/bin/pip \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Install PyTorch with CUDA support (RunPod optimized versions)
RUN pip install --no-cache-dir \
    torch==2.1.0 \
    torchvision==0.16.0 \
    --index-url https://download.pytorch.org/whl/cu121

# Install vLLM and dependencies (pinned versions for stability)
RUN pip install --no-cache-dir \
    vllm==0.2.7 \
    transformers==4.36.0 \
    accelerate \
    safetensors \
    fastapi \
    uvicorn[standard] \
    requests \
    numpy \
    sentence-transformers

# Environment variables for offline operation
ENV HF_HOME=/root/.cache/huggingface
ENV TRANSFORMERS_CACHE=/root/.cache/huggingface
ENV HF_HUB_OFFLINE=1
ENV TRANSFORMERS_OFFLINE=1
ENV CUDA_VISIBLE_DEVICES=0
ENV VLLM_USE_TRITON=0
ENV CUDA_LAUNCH_BLOCKING=0
ENV TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0"
ENV DISABLE_CUSTOM_ALL_REDUCE=1

# Create cache directory
RUN mkdir -p /root/.cache/huggingface

# Pre-download models script
RUN cat > /app/download_models.py << 'EOF'
#!/usr/bin/env python3
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
import torch

def download_model(model_name, model_type="llm"):
    print(f"Downloading {model_type} model: {model_name}")
    try:
        # Temporarily disable offline mode for downloading
        os.environ['HF_HUB_OFFLINE'] = '0'

        if model_type == "embedding":
            AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)
            AutoTokenizer.from_pretrained(model_name)
        else:  # llm
            AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
            AutoTokenizer.from_pretrained(model_name)
        print(f"✓ {model_name} downloaded successfully")

        # Re-enable offline mode
        os.environ['HF_HUB_OFFLINE'] = '1'
    except Exception as e:
        print(f"✗ Failed to download {model_name}: {e}")
        os.environ['HF_HUB_OFFLINE'] = '1'

if __name__ == "__main__":
    # Primary models for production
    download_model("openai/gpt-oss-20b", "llm")
    download_model("Qwen/Qwen3-Embedding-4B", "embedding")

    # Fallback models
    download_model("Qwen/Qwen2-0.5B-Instruct", "llm")
    download_model("BAAI/bge-small-en-v1.5", "embedding")
EOF

RUN chmod +x /app/download_models.py

# Startup script
RUN cat > /app/start_vllm.sh << 'EOF'
#!/bin/bash

# Configuration
MODEL_TYPE=${MODEL_TYPE:-"llm"}
MODEL_NAME=${MODEL_NAME:-"Qwen/Qwen2-0.5B-Instruct"}
PORT=${PORT:-8000}
TENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}
GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.7}
MAX_MODEL_LEN=${MAX_MODEL_LEN:-4096}

echo "Starting vLLM server..."
echo "Model Type: $MODEL_TYPE"
echo "Model Name: $MODEL_NAME"
echo "Port: $PORT"
echo "GPU Memory Utilization: $GPU_MEMORY_UTILIZATION"

# Check GPU
nvidia-smi || echo "No GPU detected"

# Check if model exists in cache
if [ ! -d "/root/.cache/huggingface/hub/models--$(echo $MODEL_NAME | sed 's/\//-/')" ]; then
    echo "Model not found in cache, trying to download..."
    python /app/download_models.py
fi

# Start vLLM server
exec vllm serve $MODEL_NAME \
    --host 0.0.0.0 \
    --port $PORT \
    --tensor-parallel-size $TENSOR_PARALLEL_SIZE \
    --gpu-memory-utilization $GPU_MEMORY_UTILIZATION \
    --max-model-len $MAX_MODEL_LEN \
    --trust-remote-code \
    --enforce-eager \
    --disable-custom-all-reduce
EOF

RUN chmod +x /app/start_vllm.sh

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Start vLLM
CMD ["/app/start_vllm.sh"]