# Optimized Whisper using Ivrit-AI implementation
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsndfile1 \
    curl \
    build-essential \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install PyTorch with CUDA support
RUN pip install --no-cache-dir \
    torch==2.3.0 \
    torchaudio==2.3.0 \
    --index-url https://download.pytorch.org/whl/cu121

# Install dependencies for Ivrit-AI Whisper
RUN pip install --no-cache-dir \
    fastapi==0.104.1 \
    uvicorn==0.24.0 \
    transformers==4.41.2 \
    accelerate==0.25.0 \
    safetensors>=0.4.1 \
    librosa==0.10.2 \
    python-multipart==0.0.6 \
    numpy==1.24.3 \
    optimum \
    ctranslate2 \
    faster-whisper

# Environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    HF_HOME=/root/.cache/huggingface \
    TRANSFORMERS_CACHE=/root/.cache/huggingface \
    HF_HUB_OFFLINE=1 \
    TRANSFORMERS_OFFLINE=1 \
    DEVICE=cuda \
    TORCH_DEVICE=cuda:0 \
    MODEL_NAME=ivrit-ai/whisper-large-v3-ct2

# Pre-download models
RUN python3 -c "\
import os; \
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq; \
import torch; \
model_name = os.environ.get('MODEL_NAME', 'ivrit-ai/whisper-large-v3-ct2'); \
print(f'Pre-downloading Whisper model: {model_name}'); \
processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True, cache_dir='/root/.cache/huggingface'); \
model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, torch_dtype=torch.float16, trust_remote_code=True, cache_dir='/root/.cache/huggingface'); \
print(f'Model {model_name} downloaded successfully'); \
"

# Create optimized service (based on Ivrit-AI but adapted for offline)
RUN cat > whisper_service_optimized.py << 'EOF'
import os
import torch
import logging
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, pipeline
import numpy as np
import librosa
from typing import Optional
import time
import io

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Optimized Whisper Service (Ivrit-AI Based)")

# Global variables
model = None
processor = None
pipe = None

def load_model():
    global model, processor, pipe

    model_name = os.environ.get('MODEL_NAME', 'ivrit-ai/whisper-large-v3-ct2')
    device = os.environ.get('DEVICE', 'cuda' if torch.cuda.is_available() else 'cpu')

    logger.info(f"Loading model: {model_name}")
    logger.info(f"Device: {device}")
    logger.info(f"CUDA available: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

    try:
        # Load with optimizations
        processor = AutoProcessor.from_pretrained(
            model_name,
            local_files_only=True  # Force offline mode
        )

        model = AutoModelForSpeechSeq2Seq.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
            low_cpu_mem_usage=True,
            use_safetensors=True,
            local_files_only=True  # Force offline mode
        ).to(device)

        # Create pipeline with optimizations
        pipe = pipeline(
            "automatic-speech-recognition",
            model=model,
            tokenizer=processor.tokenizer,
            feature_extractor=processor.feature_extractor,
            max_new_tokens=128,
            chunk_length_s=30,
            batch_size=16,
            return_timestamps=True,
            torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
            device=device,
        )

        logger.info("Model loaded successfully")

    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        raise

@app.on_event("startup")
async def startup_event():
    load_model()

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model": os.environ.get('MODEL_NAME', 'ivrit-ai/whisper-large-v3-ct2'),
        "device": os.environ.get('DEVICE', 'cuda' if torch.cuda.is_available() else 'cpu'),
        "cuda_available": torch.cuda.is_available(),
        "offline_mode": bool(os.environ.get('HF_HUB_OFFLINE')),
        "service": "whisper-transcription"
    }

@app.post("/transcribe")
async def transcribe(file: UploadFile = File(...)):
    if not file:
        raise HTTPException(status_code=400, detail="No file provided")

    try:
        # Read audio file
        audio_bytes = await file.read()

        # Load audio with librosa
        audio_array, sample_rate = librosa.load(
            io.BytesIO(audio_bytes),
            sr=16000  # Whisper expects 16kHz
        )

        # Transcribe with GPU acceleration
        start_time = time.time()

        with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():
            result = pipe(audio_array, generate_kwargs={"language": "he"})

        transcription_time = time.time() - start_time

        return JSONResponse(content={
            "text": result["text"],
            "language": "he",
            "processing_time": transcription_time,
            "device": str(pipe.device),
            "chunks": result.get("chunks", []),
            "model": os.environ.get('MODEL_NAME')
        })

    except Exception as e:
        logger.error(f"Transcription error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/transcribe_url")
async def transcribe_url(url: str):
    """For compatibility with Ivrit-AI RunPod API"""
    raise HTTPException(
        status_code=501,
        detail="URL transcription not supported in offline mode"
    )

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8004)
EOF

# Create startup script
RUN cat > start_whisper.sh << 'EOF'
#!/bin/bash
echo "Starting Optimized Whisper Service (Ivrit-AI Based)"
echo "Model: $MODEL_NAME"
echo "Device: $DEVICE"
echo "Offline Mode: $HF_HUB_OFFLINE"

# Ensure GPU is accessible
if [ "$DEVICE" = "cuda" ]; then
    python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
    nvidia-smi 2>/dev/null || echo "nvidia-smi not available"
fi

# Start service
exec python whisper_service_optimized.py
EOF

RUN chmod +x start_whisper.sh

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8004/health || exit 1

EXPOSE 8004

CMD ["./start_whisper.sh"]